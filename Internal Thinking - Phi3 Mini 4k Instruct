{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":61247,"databundleVersionId":8550470,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install flash-attn","metadata":{"execution":{"iopub.status.busy":"2024-07-06T10:26:22.863940Z","iopub.execute_input":"2024-07-06T10:26:22.864548Z","iopub.status.idle":"2024-07-06T10:26:52.711298Z","shell.execute_reply.started":"2024-07-06T10:26:22.864519Z","shell.execute_reply":"2024-07-06T10:26:52.710170Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.system(\"pip install -t /tmp/submission/lib flash_attn\")","metadata":{"execution":{"iopub.status.busy":"2024-07-06T10:26:52.713227Z","iopub.execute_input":"2024-07-06T10:26:52.713541Z","iopub.status.idle":"2024-07-06T10:29:04.143458Z","shell.execute_reply.started":"2024-07-06T10:26:52.713510Z","shell.execute_reply":"2024-07-06T10:29:04.142271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import snapshot_download\nfrom pathlib import Path\nimport shutil\n\nmodel_path = Path(\"/tmp/submission/\")\nif model_path.exists():\n    shutil.rmtree(model_path)\nmodel_path.mkdir(parents=True)\n\nsnapshot_download(\n    repo_id=\"microsoft/Phi-3-mini-4k-instruct\",\n    local_dir=model_path\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T10:29:04.150255Z","iopub.execute_input":"2024-07-06T10:29:04.151260Z","iopub.status.idle":"2024-07-06T10:29:29.401850Z","shell.execute_reply.started":"2024-07-06T10:29:04.151165Z","shell.execute_reply":"2024-07-06T10:29:29.400710Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile main.py\n\n# ====================ENV PATH========================\nimport os\n\nKAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\nif not os.path.exists(KAGGLE_AGENT_PATH):\n    KAGGLE_AGENT_PATH = \"/tmp/submission/\"\n\n# ====================IMPORTS========================\n\nimport sys\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(KAGGLE_AGENT_PATH, torch_dtype=\"auto\", device_map=\"cuda\")\ntokenizer = AutoTokenizer.from_pretrained(KAGGLE_AGENT_PATH)\n\nimport itertools\nimport re\nimport torch\nimport typing as t\nfrom pathlib import Path\nfrom pydantic import BaseModel, Field\n\n# ====================CONSTANTS========================\n\nstr_strip = t.Annotated[str, Field(strip_whitespace=True)]\nmodel_initialized = False\ndevice = \"cuda\"\nDEBUG = None\n\n# =========================SYSTEM MESSAGE=========================\n\nsystem_message = f\"\"\"\nYou are playing the 20 questions game. The objective is to guess the secret keyword in as few guesses as possible. \nIn this game there are thousands of possible keywords within the categories of places and things. People are not included in this game.\nThe secret keyword is randomly selected. It is very specific. It could be a country, city, food, animal, tool, appliance, furniture, etc. But not a person. \n\"\"\"\n\n# ====================INITIALIZE MODEL =========================\n\ndef initialize_model(model, tokenizer, device=device):\n    global model_initialized\n    if not model_initialized: \n        device = \"cuda\"\n        model = model.to(device)\n        tokenizer = tokenizer\n        model_initialized = True  \n        \n# =========================DATA MODEL=========================\n\nclass Observation(BaseModel):\n    step: int\n    role: t.Literal[\"guesser\", \"answerer\"]\n    turnType: t.Literal[\"ask\", \"answer\", \"guess\"]\n    keyword: str\n    category: str\n    questions: list[str]\n    answers: list[str]\n    guesses: list[str]\n    \n    @property\n    def empty(self) -> bool:\n        return all(len(t) == 0 for t in [self.questions, self.answers, self.guesses])\n    \n    def get_history(self) -> t.Iterator[tuple[str, str, str]]:\n        return itertools.zip_longest(self.questions, self.answers, self.guesses, fillvalue=\"[none]\")\n\n    def get_history_as_xml(self, *, skip_guesses: bool = False, skip_answer: bool = False, skip_question: bool = False) -> str:\n        if not self.empty:\n            history = \"\\n\".join(\n            f\"\"\"\\\n            {'Question: ' + question if not skip_question else ''}\n            {'Answer: ' + answer if not skip_answer else ''}\n            {'Guess: ' + guess if not skip_guesses else ''}\n            \"\"\"\n            for i, (question, answer, guess) in enumerate(self.get_history())\n            )\n            return history\n        return \"none yet.\"\n\n\n# =========================ASK AGENT=========================\n\ndef ask(observation: Observation) -> str:\n    if observation.step == 0:\n        return \"Is the secret keyword a place?\"\n    elif observation.step == 1:\n        if len(observation.answers) > 0 and observation.answers[0] == \"yes\":\n            return \"Is it in the western hemisphere?\"\n        else:\n            return \"Is it a living thing?\"\n    elif observation.step == 2:\n        if len(observation.answers) > 0 and observation.answers[0] == \"yes\":\n            return \"Is the place natural and not man-made?\"\n        else:\n            return \"Is it related to food or drinks?\"\n\n    branch = determine_branch(observation)\n    \n\n    try:\n        hx_text = assess_history(branch, observation)\n        if DEBUG:\n            print(\"DEBUG_____ASSESS HISTROY PROMPT (QUESTIONER)\", hx_text)\n        hx_text = generate_text_long(hx_text)\n        chain_of_thought_text = thinking(hx_text, observation)\n        if DEBUG:\n            print(\"DEBUG_____THINKING PROMPT (QUESTIONER)\", chain_of_thought_text)\n        chain_of_thought_text = generate_text_long(chain_of_thought_text)\n        response = chain_of_thought_text.lower()\n        if DEBUG:\n            print(\"DEBUG_____QUESTION PASRING PROMPT (QUESTIONER)\", response)\n        q_parsed = parse_response_generation(response)\n        return q_parsed\n    except Exception as e:\n        print(f\"Error while asking question: {e}\")\n        return 'Is it a person?'\n\ndef determine_branch(observation: Observation) -> str:\n    \"\"\"\n    Determine the branch based on the Q&A history.\n    \"\"\"\n    place_questions = {\n        \"Is it in the western hemisphere?\": \"Place in Western Hemisphere\",\n        \"Is the place natural and not man-made?\": \"Natural place, not man-made\",\n    }\n    \n    thing_questions = {\n        \"Is it a living thing?\": \"a living thing\",\n        \"Is it related to food?\": \"related to food or drink\",\n    }\n\n    for question, answer in zip(observation.questions, observation.answers):\n        if observation.questions[0] == \"Is the secret keyword a place?\" and observation.answers[0] == \"yes\":\n            if question in place_questions and answer == \"yes\":\n                return place_questions[question]\n        elif observation.questions[0] == \"Is the secret keyword a place?\" and observation.answers[0] == \"no\":\n            if question in thing_questions and answer == \"yes\":\n                return thing_questions[question]\n    \n    if observation.questions[0] == \"Is the secret keyword a place?\" and observation.answers[0] == \"yes\":\n        return \"place\"\n    else:\n        return \"thing\"\n    \ndef assess_history(branch, observation):\n    prompt = f\"\"\"\n    Your objective is to review the following game history and create a summary of what is known and what is not known about the keyword. The guesses inckuded are not correct, so we are still playing the game.\n    Here is the history:\n\n    {observation.get_history_as_xml(skip_guesses=False)}\n    \n    Now provide the follwing information:\n    - What is known about the keyword?\n    - What is not known about the keyword?\n    - Make a list of the incorrect guesses, if any. We know that these are incorrect since the game is still in progress. \n    - We know the the keyword is a {branch} based on the questions asked and the answers provided. What specific {branch} is the keyword?\n    \"\"\"\n    return prompt\n\ndef thinking(hx_text, observation):\n    prompt = f\"\"\"\n    You are thinking about what question to ask next. You have the following information to consider:\n    \n    {hx_text}\n\n    Based on the information above, what is the next question you would like to ask to bisect the search space and narrow down the possibilities?\n    If categories are still unclear, it may be best to ask a yes/no question that narrows the known category down further. You are not allowed to ask questions that are not yes/no.\n    Your response should start with your reasoning, then you say therefore my next quesiton is:\n    \"\"\"\n    return prompt\n\ndef parse_question(response:str) -> str:\n    prompt = f\"\"\"\n    Your role is to take text below and parse a single valid question for the game of 20 questions. You do not provide extra details. Just the question based on the text provided.\n    It should be formatted as a question and end with a question mark. Limit the question to 100 characters. If a list is provided, use the first item. \n    Here is the text:\n    \n    \"{response}\"\n\n    Reply with the single question.\n    \"\"\"\n    \n#=========================ANSWER AGENT=========================\n\ndef answer(observation: \"Observation\") -> t.Literal[\"yes\", \"no\"]:\n    if not observation.keyword:\n        print(\"Keyword wasn't provided to answerer\", file=sys.stderr)\n        return \"yes\"  # Default to \"yes\" if keyword is missing.\n            \n    last_question = observation.questions[-1]\n\n    try:\n        responses = []\n        for i in range(5):\n            response = generate_text_short(\n                f\"\"\"\\\n                Answer yes/no for this keyword: {observation.keyword}\n                Question: {last_question}\n\n                Rules:\n                1. Only consider your keyword: {observation.keyword}\n                2. Think about the context of your keyword and the question asked about it.\n                3. Answer only 'yes' or 'no'\n                Your answer:\n                \"\"\"\n            ).strip().lower() \n            if response in {\"yes\", \"no\"}:\n                responses.append(response)\n            else:\n                if DEBUG:\n                    print(f\"DEBUG____INVALID ANSWER (ANSWERER): '{response}'\")\n\n        if DEBUG:\n            print(\"DEBUG____ALL ANSWERS (ANSWERER): \", responses)\n\n        if responses:\n            return max(set(responses), key=responses.count)\n        else:\n            return \"yes\"\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\", file=sys.stderr)\n        return \"yes\"\n    \n# =========================GUESS AGENT=========================\n\ndef guess(observation: Observation) -> str:\n\n    try:\n        branch = determine_branch(observation)\n        hx_text = assess_history_for_guess(branch, observation)\n        if DEBUG:\n            print(\"DEBUG____ASSESS FOR HISTORY PROMPT (GUESSER): \", hx_text)\n        hx_text = generate_text_long(hx_text)\n        thinking_text = thinking_for_guess(hx_text)\n        if DEBUG:\n            print(\"DEBUG_____THINKING PROMPT (GUESSER): \", thinking_text)\n        thinking_text = generate_text_long(thinking_text)\n        response = thinking_text.lower()\n        guess = parse_guess(response)\n        if DEBUG:\n            print(\"DEBUG____GUESS PARSE PROMPT (GUESSER): \", guess)\n        guess = parse_response_generation(guess)\n        return guess\n    except Exception as e:\n        print(f\"Error while making guess: {e}\")\n        return 'france'\n    \ndef assess_history_for_guess(branch, observation):\n    prompt = f\"\"\"\n    Your objective is to review the following game history and create a summary of what is known and what is not known about the keyword. The guesses inckuded are not correct, so we are still playing the game.\n    Here is the history:\n\n    {observation.get_history_as_xml(skip_guesses=False)}\n    \n    Now provide the follwing information:\n    - What is known about the keyword?\n    - What is not known about the keyword?\n    - Make a list of the previous incorrect guesses, if any. We know that these are incorrect since the game is still in progress.\n    - We know the the keyword is a {branch} based on the questions asked and the answers provided. What specific {branch} is the keyword?\n    - List 10 possible keywords that could be the secret keyword at the end. In early game, it could be anything but as the game progresses, the list should be narrowed down.\n    - Make sure the keywords are specific and not too general. If you only have limited information, make wild guesses.\n    \"\"\"\n    return prompt\n\ndef thinking_for_guess(hx_text):\n    prompt = f\"\"\"\n    You are thinking about what specific guess to make next. You have the following information to consider:\n    \n    {hx_text}\n\n    Based on the information above, what is the best guess you can make? You are required to make a guess, regardless of little information.\n    Your response should start with your reasoning, then you say therefore my next guess is:\n    \"\"\"\n    return prompt\n\ndef parse_guess(response:str) -> str:\n    prompt = f\"\"\"\n    Your role is to take text below and parse a valid guess for the game of 20 questions. You do not provide extra details. Just the guess based on the text provided.\n    It should be formatted as a guess and have no punctuation. Limit the guess to 50 characters.\n    Here is the text:\n\n    \"{response}\"\n\n    Reply with the word or phrase that is the guess here:\n    \"\"\"\n    return prompt\n\n# =========================GENERATE TEXT=========================\n\ndef generate_text_long(prompt:str) -> str:\n    global model_initialized\n    if not model_initialized:\n        initialize_model(model, tokenizer, device=device)\n    sys_prompt = system_message\n    messages = [\n        {\"role\": \"system\", \"content\": sys_prompt},\n        {\"role\": \"user\", \"content\": prompt},\n    ]\n    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    inputs = tokenizer([text], return_tensors=\"pt\", truncation=True).to(device)\n    generated_ids = model.generate(inputs.input_ids, max_new_tokens=500, temperature=0.75, do_sample=True)\n    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, generated_ids)]\n    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n    return response\n\ndef generate_text_short(prompt:str) -> str:\n    global model_initialized\n    if not model_initialized:\n        initialize_model(model, tokenizer, device=device)\n    sys_prompt = system_message\n    messages = [\n        {\"role\": \"system\", \"content\": sys_prompt},\n        {\"role\": \"user\", \"content\": prompt},\n    ]\n    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    inputs = tokenizer([text], return_tensors=\"pt\", truncation=True).to(device)\n    generated_ids = model.generate(inputs.input_ids, max_new_tokens=150, temperature=0.4, do_sample=True)\n    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, generated_ids)]\n    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n    return response\n    \ndef parse_response_generation(prompt:str) -> str:\n    global model_initialized\n    if not model_initialized:\n        initialize_model(model, tokenizer, device=device)\n    messages = [\n        {\"role\": \"system\", \"content\": \"Your role is to take text and parse a valid question or guess for the game of 20 questions. You do not provide extra details. Just the question or the answer based on the text provided.\"},\n        {\"role\": \"user\", \"content\": prompt},\n    ]\n    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    inputs = tokenizer([text], return_tensors=\"pt\", truncation=True).to(device)\n    generated_ids = model.generate(inputs.input_ids, max_new_tokens=30, temperature=0.1, do_sample=True)\n    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, generated_ids)]\n    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n    return response\n\nagent = None\n\ndef observe(obs: t.Any) -> str:\n    global agent\n    observation = Observation(**obs.__dict__)\n\n    try:\n        match observation.turnType:\n            case \"ask\":\n                agent = ask(observation)\n                return agent\n            case \"answer\":\n                agent = answer(observation)\n                return agent\n            case \"guess\":\n                agent = guess(observation)\n                return agent\n\n            case _:\n                raise ValueError(\"Unknown turn type\")\n    except Exception as e:\n        print(str(e), file=sys.stderr)\n        raise\n\ndef agent_fn(obs: t.Any, _: t.Any) -> str:\n    return observe(obs)\n\n# =========================ENABLE TORCH BACKEND===========================\n\ntorch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-06T11:25:40.732982Z","iopub.execute_input":"2024-07-06T11:25:40.733328Z","iopub.status.idle":"2024-07-06T11:25:40.788444Z","shell.execute_reply.started":"2024-07-06T11:25:40.733302Z","shell.execute_reply":"2024-07-06T11:25:40.787565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission\nUncomment for submission to Kaggle","metadata":{}},{"cell_type":"code","source":"!apt install pigz pv","metadata":{"execution":{"iopub.status.busy":"2024-07-06T10:29:36.834281Z","iopub.execute_input":"2024-07-06T10:29:36.835033Z","iopub.status.idle":"2024-07-06T10:29:37.931619Z","shell.execute_reply.started":"2024-07-06T10:29:36.834999Z","shell.execute_reply":"2024-07-06T10:29:37.930473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!tar --use-compress-program='pigz --fast' \\\n     -cf submission.tar.gz \\\n     -C /kaggle/working main.py \\\n     -C /tmp/submission/ \\\n     .","metadata":{"execution":{"iopub.status.busy":"2024-07-06T10:29:37.933126Z","iopub.execute_input":"2024-07-06T10:29:37.933409Z","iopub.status.idle":"2024-07-06T10:29:38.010205Z","shell.execute_reply.started":"2024-07-06T10:29:37.933385Z","shell.execute_reply":"2024-07-06T10:29:38.008990Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation and Debugging","metadata":{}},{"cell_type":"code","source":"# import time\n# import typing as t\n\n# DEBUG = input(\"Enable debug mode? (y/n) \") == 'y'\n\n# class MockObservation:\n#     def __init__(self, step: int, role: str, turnType: str, keyword: str, category: str, questions: list[str], answers: list[str], guesses: list[str]):\n#         self.step = step\n#         self.role = role\n#         self.turnType = turnType\n#         self.keyword = keyword\n#         self.category = category\n#         self.questions = questions\n#         self.answers = answers\n#         self.guesses = guesses\n\n# def test_20_questions():\n#     global DEBUG\n#     step = 0\n#     role = \"answerer\"\n#     turnType = \"ask\"\n#     keyword = \"yoga mat\"\n#     category = \"\"\n#     questions = []\n#     answers = []\n#     guesses = []\n#     print(\"Starting 20 questions eval game...\")\n#     print(\"Keyword:\", keyword)\n    \n#     for i in range(60):\n#         obs = MockObservation(step, role, turnType, keyword, category, questions, answers, guesses)\n        \n#         start_time = time.time()\n#         response = agent_fn(obs, None)\n#         end_time = time.time()\n        \n#         response_time = end_time - start_time\n        \n#         # Record the response in the appropriate list\n#         if turnType == 'ask':\n#             questions.append(response)\n#             turnType = 'answer'\n#         elif turnType == 'answer':\n#             answers.append(response)\n#             turnType = 'guess'\n#         elif turnType == 'guess':\n#             guesses.append(response)\n#             if response.lower() == keyword.lower():\n#                 print(f\"Keyword '{keyword}' guessed correctly! Ending game.\")\n#                 break\n#             turnType = 'ask'\n#             step += 1\n#             role = 'guesser' if role == 'answerer' else 'answerer'\n        \n#         print(f\"{response} {response_time:.2f} sec\")\n#     print(\"Secret Keyword:\", keyword)\n#     print(\"Final Questions:\", questions)\n#     print(\"Final Answers:\", answers)\n#     print(\"Final Guesses:\", guesses)\n\n# # Run the test\n# test_20_questions()","metadata":{"execution":{"iopub.status.busy":"2024-07-06T11:26:09.838254Z","iopub.execute_input":"2024-07-06T11:26:09.838627Z","iopub.status.idle":"2024-07-06T11:29:37.400543Z","shell.execute_reply.started":"2024-07-06T11:26:09.838598Z","shell.execute_reply":"2024-07-06T11:29:37.398868Z"},"trusted":true},"execution_count":null,"outputs":[]}]}